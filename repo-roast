#!/usr/bin/env python3
"""
Puts a GitHub repo on a spit and slowly rotates it over an open flame.

Usage:
  repo-roast owner/repo
  repo-roast https://github.com/owner/repo
  repo-roast owner/repo --model sonnet
"""

import argparse
import asyncio
import base64
import json
import re
import subprocess
import sys
from datetime import datetime, timezone

try:
    from rich.console import Console, Group
    from rich.live import Live
    from rich.markdown import Markdown
    from rich.panel import Panel
    from rich.text import Text
    from rich.table import Table
except ImportError:
    print("Missing dependency: rich\nInstall: pip install rich")
    sys.exit(1)

console = Console()


def check_dependency(cmd):
    """Verify a CLI tool is installed."""
    try:
        subprocess.run([cmd, "--version"], capture_output=True, timeout=10)
        return True
    except FileNotFoundError:
        return False


def parse_repo(arg):
    """Extract owner/repo from URL or direct reference."""
    m = re.match(r"https?://github\.com/([^/]+/[^/]+?)(?:/.*)?$", arg)
    if m:
        return m.group(1).rstrip("/")
    if re.match(r"^[^/]+/[^/]+$", arg):
        return arg
    return None


async def gh(endpoint):
    """Single GitHub API call via gh CLI."""
    proc = await asyncio.create_subprocess_exec(
        "gh", "api", endpoint,
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.PIPE,
    )
    stdout, _ = await proc.communicate()
    if proc.returncode != 0:
        return None
    text = stdout.decode()
    return json.loads(text) if text.strip() else None


async def gh_list(endpoint):
    """Paginated GitHub API call via gh CLI."""
    proc = await asyncio.create_subprocess_exec(
        "gh", "api", "--paginate", "--jq", ".[]", endpoint,
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.PIPE,
    )
    stdout, _ = await proc.communicate()
    if proc.returncode != 0:
        return []
    text = stdout.decode().strip()
    if not text:
        return []
    items = []
    for line in text.split("\n"):
        line = line.strip()
        if line:
            try:
                items.append(json.loads(line))
            except json.JSONDecodeError:
                continue
    return items


async def fetch_all(repo_name):
    """Fetch all repo data concurrently."""
    data = {"errors": []}

    repo = await gh(f"/repos/{repo_name}")
    if not repo:
        return None
    data["repo"] = repo

    default_branch = repo.get("default_branch", "main")

    results = await asyncio.gather(
        gh(f"/repos/{repo_name}/languages"),
        gh_list(f"/repos/{repo_name}/contributors?per_page=100&anon=true"),
        gh_list(f"/repos/{repo_name}/releases?per_page=5"),
        gh(f"/repos/{repo_name}/stats/participation"),
        gh_list(f"/repos/{repo_name}/commits?per_page=30"),
        gh(f"/repos/{repo_name}/community/profile"),
        gh(f"/repos/{repo_name}/git/trees/{default_branch}"),
        gh(f"/repos/{repo_name}/readme"),
    )

    languages, contribs, releases, participation, recent_commits, community, tree, readme_resp = results

    if languages is None:
        data["errors"].append("languages")
    data["languages"] = languages or {}
    data["contributors"] = contribs or []
    data["releases"] = releases or []
    data["participation"] = participation
    data["recent_commits"] = recent_commits or []
    data["community"] = community
    data["tree"] = tree

    if readme_resp and "content" in readme_resp:
        content = readme_resp["content"].replace("\n", "")
        data["readme"] = base64.b64decode(content).decode("utf-8", errors="replace")
    else:
        data["readme"] = None

    return data


async def claude_analyze(prompt, model="opus", timeout=180):
    """Send prompt to claude CLI, return text response."""
    try:
        proc = await asyncio.create_subprocess_exec(
            "claude", "-p", "--model", model, "--output-format", "json",
            stdin=asyncio.subprocess.PIPE,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )
        stdout, _ = await asyncio.wait_for(
            proc.communicate(input=prompt.encode()), timeout=timeout
        )
    except asyncio.TimeoutError:
        return None
    if proc.returncode != 0:
        return None
    text = stdout.decode()
    try:
        output = json.loads(text)
        return output.get("result", text) if isinstance(output, dict) else text
    except json.JSONDecodeError:
        return text


def print_metrics(data):
    """Print metrics as a Rich table in a panel."""
    repo = data["repo"]
    languages = data["languages"]
    now = datetime.now(timezone.utc)
    pushed = datetime.fromisoformat(repo["pushed_at"].replace("Z", "+00:00"))
    created = datetime.fromisoformat(repo["created_at"].replace("Z", "+00:00"))
    age_days = (now - created).days
    push_days = (now - pushed).days

    total_bytes = sum(languages.values()) if languages else 0
    lang_breakdown = "none"
    if languages and total_bytes > 0:
        lang_breakdown = ", ".join(
            f"{k} {v * 100 / total_bytes:.1f}%"
            for k, v in sorted(languages.items(), key=lambda x: -x[1])
        )

    license_id = "none"
    if repo.get("license") and isinstance(repo["license"], dict):
        license_id = repo["license"].get("spdx_id", "none")

    contrib_count = len(data["contributors"])
    release_count = len(data["releases"])

    commit_note = f"{len(data['recent_commits'])} in last 30 fetched"
    if data["participation"]:
        yearly = sum(data["participation"].get("all", []))
        commit_note = f"~{yearly} in past year"

    community = data.get("community")
    health_pct = f"{community.get('health_percentage', '?')}%" if community else "?"

    table = Table.grid(padding=(0, 2))
    table.add_column(style="bold cyan", justify="right", min_width=14)
    table.add_column(min_width=18)
    table.add_column(style="bold cyan", justify="right", min_width=14)
    table.add_column(min_width=18)

    rows = [
        ("Stars", str(repo.get("stargazers_count", 0))),
        ("Forks", str(repo.get("forks_count", 0))),
        ("Watchers", str(repo.get("subscribers_count", 0))),
        ("Contributors", str(contrib_count)),
        ("Open issues", str(repo.get("open_issues_count", 0))),
        ("Commits", commit_note),
        ("Created", f"{repo['created_at'][:10]}"),
        ("Last push", f"{repo['pushed_at'][:10]}"),
        ("Size", f"{repo.get('size', 0)} KB"),
        ("License", license_id),
        ("Releases", str(release_count)),
        ("Archived", str(repo.get("archived", False))),
        ("Health", health_pct)
    ]

    for i in range(0, len(rows), 2):
        r1 = rows[i]
        if i + 1 < len(rows):
            r2 = rows[i+1]
            table.add_row(r1[0], r1[1], r2[0], r2[1])
        else:
            table.add_row(r1[0], r1[1], "", "")

    # Group table and extra info
    elements = [table]
    if lang_breakdown != "none":
        elements.append(Text(f"\nLanguages: {lang_breakdown}", style="dim"))
    if repo.get("topics"):
        elements.append(Text(f"Topics: {', '.join(repo.get('topics', []))}", style="dim"))

    console.print(Panel(Group(*elements), title="[bold]Metrics[/bold]", border_style="dim", safe_box=True))

    if data["errors"]:
        console.print(f"  [dim yellow]Fetch warnings: {', '.join(data['errors'])}[/dim yellow]")


def build_prompt(data):
    """Build the LLM analysis prompt from raw data."""
    repo = data["repo"]
    languages = data["languages"]
    now = datetime.now(timezone.utc)
    pushed = datetime.fromisoformat(repo["pushed_at"].replace("Z", "+00:00"))
    created = datetime.fromisoformat(repo["created_at"].replace("Z", "+00:00"))

    total_bytes = sum(languages.values()) if languages else 0
    lang_breakdown = ""
    if languages and total_bytes > 0:
        lang_breakdown = ", ".join(
            f"{k} {v * 100 / total_bytes:.1f}%"
            for k, v in sorted(languages.items(), key=lambda x: -x[1])
        )

    license_id = "none"
    if repo.get("license") and isinstance(repo["license"], dict):
        license_id = repo["license"].get("spdx_id", "none")

    # Commit activity & cadence
    commit_info = "unknown"
    cadence_note = "insufficient data"
    if data["participation"]:
        weekly = data["participation"].get("all", [])
        yearly = sum(weekly)
        recent_12w = sum(weekly[-12:]) if len(weekly) >= 12 else sum(weekly)
        commit_info = f"~{yearly} in past year, ~{recent_12w} in last 12 weeks"
        if weekly:
            active_weeks = sum(1 for w in weekly if w > 0)
            prior_12w = sum(weekly[-24:-12]) if len(weekly) >= 24 else 0
            cadence_note = (
                f"{active_weeks}/{len(weekly)} weeks active; "
                f"{recent_12w} commits in last 12 weeks"
            )
            if prior_12w > 0:
                trend = "stable"
                if recent_12w > prior_12w * 1.25:
                    trend = "accelerating"
                elif recent_12w < prior_12w * 0.75:
                    trend = "decelerating"
                cadence_note += f" vs {prior_12w} in prior 12 ({trend})"

    # Recent commit authors
    author_counts = {}
    for c in data["recent_commits"][:20]:
        author = c.get("author", {})
        login = author.get("login", "unknown") if author else "unknown"
        author_counts[login] = author_counts.get(login, 0) + 1
    top_authors = ", ".join(
        f"{k} ({v})" for k, v in
        sorted(author_counts.items(), key=lambda x: -x[1])[:5]
    )

    # Bus factor
    bus_factor_note = "insufficient data"
    if author_counts:
        total_recent = sum(author_counts.values())
        top_author = max(author_counts.items(), key=lambda x: x[1])
        top_pct = top_author[1] / total_recent * 100
        bus_factor_note = (
            f"Top contributor ({top_author[0]}) authored {top_pct:.0f}% "
            f"of last {total_recent} sampled commits; "
            f"{len(author_counts)} distinct authors"
        )

    # Release info
    release_info = "none"
    if data["releases"]:
        names = [r.get("tag_name", "?") for r in data["releases"][:5]]
        latest_date = data["releases"][0].get("published_at", "?")[:10] if data["releases"] else "?"
        release_info = f"{len(data['releases'])} releases, latest: {names[0]} ({latest_date}), recent tags: {', '.join(names)}"

    # Community profile
    community_info = "unknown"
    if data["community"]:
        files = data["community"].get("files", {})
        present = [k for k, v in files.items() if v]
        community_info = f"health {data['community'].get('health_percentage', '?')}%, has: {', '.join(present) or 'nothing'}"

    # File tree (top-level)
    tree_info = "unavailable"
    if data["tree"] and "tree" in data["tree"]:
        entries = data["tree"]["tree"]
        dirs = [e["path"] for e in entries if e["type"] == "tree"]
        files = [e["path"] for e in entries if e["type"] == "blob"]
        tree_info = f"Directories: {', '.join(dirs[:20])}\nFiles: {', '.join(files[:20])}"
        if len(dirs) > 20 or len(files) > 20:
            tree_info += f"\n(truncated, {len(dirs)} dirs and {len(files)} files total)"

    # Structural signals from file tree
    structure_signals = []
    if data["tree"] and "tree" in data["tree"]:
        entries = data["tree"]["tree"]
        dir_names = {e["path"].lower() for e in entries if e["type"] == "tree"}
        file_names = {e["path"].lower() for e in entries if e["type"] == "blob"}
        has_tests = bool(dir_names & {"tests", "test", "__tests__", "spec", "specs"})
        has_ci = bool(dir_names & {".github", ".circleci"}) or bool(
            file_names & {".travis.yml", "jenkinsfile", ".gitlab-ci.yml", "azure-pipelines.yml"}
        )
        has_docs = bool(dir_names & {"docs", "doc", "documentation"})
        has_docker = any(
            f.startswith("dockerfile") or f in ("docker-compose.yml", "docker-compose.yaml")
            for f in file_names
        )
        structure_signals = [
            f"Tests: {'present' if has_tests else 'not detected'}",
            f"CI/CD: {'present' if has_ci else 'not detected'}",
            f"Docs directory: {'present' if has_docs else 'not detected'}",
            f"Containerization: {'present' if has_docker else 'not detected'}",
        ]

    # README
    readme_section = "No README available."
    if data["readme"]:
        readme_text = data["readme"]
        total_len = len(readme_text)
        max_len = 4000
        if total_len > max_len:
            readme_text = readme_text[:max_len]
            readme_section = f"README (truncated to {max_len} of {total_len} chars):\n{readme_text}"
        else:
            readme_section = f"README ({total_len} chars):\n{readme_text}"

    # README hype term analysis
    buzzword_note = "no README to analyze"
    if data["readme"]:
        readme_lower = data["readme"].lower()
        hype_terms = [
            "revolutionary", "groundbreaking", "cutting-edge", "state-of-the-art",
            "enterprise-grade", "production-ready", "blazing fast", "next-generation",
            "game-changing", "world-class", "best-in-class", "industry-leading",
            "paradigm shift", "disruptive", "democratize", "empower",
            "turnkey", "mission-critical", "battle-tested", "seamless",
            "zero-config", "drop-in replacement", "superhuman",
            "10x", "100x", "1000x", "magic", "automagic", "effortless",
            "no-brainer",
        ]
        found = {}
        for term in hype_terms:
            count = readme_lower.count(term)
            if count:
                found[term] = count
        total_found = sum(found.values())
        if found:
            top_terms = ", ".join(
                f'"{t}" ({c})' for t, c in
                sorted(found.items(), key=lambda x: -x[1])[:8]
            )
            buzzword_note = f"{total_found} hype terms detected: {top_terms}"
        else:
            buzzword_note = "0 hype terms detected"

    prompt = (
        "You are a senior software engineering analyst. Produce a structured, "
        "evidence-based repository assessment. Rules:\n"
        "1. Every claim must reference specific data points from the metrics below.\n"
        "2. Interpret what numbers mean — do not parrot them back.\n"
        "3. Do not speculate beyond what the data supports.\n"
        "4. Do not soften negative findings. Accuracy is the only priority.\n"
        "5. Compare metrics against reasonable baselines for projects of similar "
        "type, scope, and age.\n\n"
        "Produce exactly these sections:\n\n"
        "## Project Vitals\n"
        "Assess project health across these dimensions (2-4 sentences total):\n"
        "- **Sustainability**: Bus factor, contributor distribution, commit "
        "cadence consistency (active weeks vs total span), trajectory "
        "(accelerating, decelerating, or stalled)\n"
        "- **Traction**: Stars and forks relative to age and project category; "
        "watcher-to-star ratio as a genuine-interest signal; organic vs "
        "campaign-driven growth patterns\n"
        "- **Maturity**: Release discipline, versioning practices, license "
        "presence, community health score interpreted in context\n\n"
        "## Technical Assessment\n"
        "Evaluate engineering rigor from observable artifacts (3-5 sentences):\n"
        "- Does the repository structure reflect professional practices? "
        "Evaluate presence or absence of tests, CI/CD, documentation, and "
        "modular organization.\n"
        "- Compare the project's stated scope and claims against actual "
        "codebase size, language distribution, commit volume, and structural "
        "complexity.\n"
        "- Assess README substance: ratio of marketing language to actionable "
        "technical content; unsubstantiated claims; whether examples match "
        "stated capabilities.\n\n"
        "## Risk Assessment\n"
        "Identify concrete risks for someone considering adoption (2-4 sentences):\n"
        "- **Maintenance**: Abandonment probability, bus factor implications, "
        "issue backlog\n"
        "- **Credibility**: Self-referential benchmarks, scope inflation, "
        "claims without evidence\n"
        "- **Operational**: Licensing gaps, missing versioned releases, absent "
        "CI/CD, security posture\n\n"
        "## Functional Summary\n"
        "In 2-3 sentences, describe what this project technically does. Name "
        "specific technologies, libraries, patterns, and architectural choices "
        "evident in the data. Describe mechanisms, not aspirations.\n\n"
        "## Verdict\n"
        "One sentence. Maximum precision, no filler.\n\n"
        "---\n\n"
        "REPOSITORY DATA\n\n"
        f"Repo: {repo.get('full_name', '?')}\n"
        f"Description: {repo.get('description', 'none')}\n"
        f"Stars: {repo.get('stargazers_count', 0)} | "
        f"Forks: {repo.get('forks_count', 0)} | "
        f"Watchers: {repo.get('subscribers_count', 0)}\n"
        f"Contributors: {len(data['contributors'])}\n"
        f"Open issues (includes PRs): {repo.get('open_issues_count', 0)}\n"
        f"Age: {(now - created).days} days | "
        f"Last push: {(now - pushed).days} days ago\n"
        f"Commits: {commit_info}\n"
        f"Commit cadence: {cadence_note}\n"
        f"Recent commit authors: {top_authors or 'unknown'}\n"
        f"Bus factor: {bus_factor_note}\n"
        f"Languages: {lang_breakdown or 'none'}\n"
        f"License: {license_id}\n"
        f"Releases: {release_info}\n"
        f"Community profile: {community_info}\n"
        f"Archived: {repo.get('archived', False)}\n"
        f"Structure signals: {'; '.join(structure_signals) if structure_signals else 'unavailable'}\n"
        f"README hype analysis: {buzzword_note}\n\n"
        f"Top-level file tree:\n{tree_info}\n\n"
        f"{readme_section}"
    )
    return prompt


async def main():
    p = argparse.ArgumentParser(
        description="Roasts GitHub repos with data and LLM analysis",
        usage="repo-roast <owner/repo | github-url> [options]",
    )
    p.add_argument("repo", help="owner/repo or GitHub URL")
    p.add_argument("--model", default="opus", help="Claude model (default: opus)")
    p.add_argument("--no-llm", action="store_true", help="Metrics only, skip LLM analysis")
    p.add_argument("--json", action="store_true", help="Raw JSON output")
    args = p.parse_args()

    if not check_dependency("gh"):
        console.print("[red]Error:[/red] gh CLI not found. Install from https://cli.github.com/")
        sys.exit(1)
    if not args.no_llm and not check_dependency("claude"):
        console.print("[red]Error:[/red] claude CLI not found. Install from https://docs.anthropic.com/en/docs/claude-code")
        sys.exit(1)

    repo_name = parse_repo(args.repo)
    if not repo_name:
        console.print(f"[red]Error:[/red] can't parse '{args.repo}' — use owner/repo or a GitHub URL")
        sys.exit(1)

    console.print(f"\n[bold red]:fire: Roasting[/bold red] [bold]{repo_name}[/bold]\n")

    # Fetch (all API calls run concurrently)
    with console.status("[dim]Gathering data...[/dim]", spinner="simpleDots"):
        data = await fetch_all(repo_name)
    if not data:
        console.print(f"[red]Error:[/red] could not fetch {repo_name} (not found or no access)")
        sys.exit(1)

    # JSON output mode
    if args.json:
        out = {
            "repo": data["repo"],
            "languages": data["languages"],
            "contributors": len(data["contributors"]),
            "releases": len(data["releases"]),
            "errors": data["errors"],
        }
        print(json.dumps(out, indent=2))
        return

    print_metrics(data)

    if args.no_llm:
        console.print("\n[bold red]:fire: Roasted.[/bold red]\n")
        return

    # LLM analysis
    with console.status(f"[dim]Claude ({args.model}) is thinking...[/dim]", spinner="simpleDots"):
        prompt = build_prompt(data)
        result = await claude_analyze(prompt, model=args.model, timeout=180)

    if result:
        console.print()
        
        # Slow scroll effect
        lines = result.strip().splitlines()
        current_content = ""
        with Live(console=console, refresh_per_second=20) as live:
            for line in lines:
                current_content += line + "\n"
                live.update(Panel(
                    Markdown(current_content),
                    title=f"[bold]The Roast[/bold] [dim]({args.model})[/dim]",
                    border_style="red",
                    safe_box=True,
                ))
                await asyncio.sleep(0.08)  # Slow scroll pace

        console.print("\n[bold red]:fire: Roasted.[/bold red]\n")
        # Hold on the ending for 5 seconds (useful for asciinema recordings)
        await asyncio.sleep(5)
    else:
        console.print("[red]Error:[/red] claude did not return a response")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
